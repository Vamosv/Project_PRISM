{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRISM Sample Dataset Shape: (154, 41)\n",
      "Number of unique conversations: 154\n",
      "Number of unique participants: 137\n",
      "\n",
      "Conversations per participant:\n",
      "  Mean: 1.12\n",
      "  Median: 1.00\n",
      "  Min: 1\n",
      "  Max: 3\n",
      "âš  Note: Not all participants have exactly 6 conversations.\n",
      "Participants with != 6 conversations: 137\n",
      "\n",
      "=== DEMOGRAPHIC DISTRIBUTIONS ===\n",
      "\n",
      "GENDER DISTRIBUTION:\n",
      "gender\n",
      "Male                         0.496350\n",
      "Female                       0.496350\n",
      "Non-binary / third gender    0.007299\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "AGE DISTRIBUTION:\n",
      "age\n",
      "25-34 years old    0.277372\n",
      "35-44 years old    0.189781\n",
      "55-64 years old    0.175182\n",
      "18-24 years old    0.160584\n",
      "45-54 years old    0.124088\n",
      "65+ years old      0.072993\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "EDUCATION DISTRIBUTION:\n",
      "education\n",
      "University Bachelors Degree       0.416058\n",
      "Graduate / Professional degree    0.189781\n",
      "Some University but no degree     0.160584\n",
      "Completed Secondary School        0.153285\n",
      "Vocational                        0.058394\n",
      "Some Secondary                    0.021898\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "EMPLOYMENT_STATUS DISTRIBUTION:\n",
      "employment_status\n",
      "Working full-time                  0.437956\n",
      "Working part-time                  0.182482\n",
      "Student                            0.109489\n",
      "Unemployed, seeking work           0.102190\n",
      "Unemployed, not seeking work       0.065693\n",
      "Retired                            0.051095\n",
      "Homemaker / Stay-at-home parent    0.029197\n",
      "Prefer not to say                  0.021898\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "MARITAL_STATUS DISTRIBUTION:\n",
      "marital_status\n",
      "Never been married      0.547445\n",
      "Married                 0.335766\n",
      "Divorced / Separated    0.094891\n",
      "Prefer not to say       0.014599\n",
      "Widowed                 0.007299\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ENGLISH_PROFICIENCY DISTRIBUTION:\n",
      "english_proficiency\n",
      "Native speaker    0.656934\n",
      "Fluent            0.197080\n",
      "Advanced          0.102190\n",
      "Intermediate      0.043796\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LM_FAMILIARITY DISTRIBUTION:\n",
      "lm_familiarity\n",
      "Somewhat familiar      0.766423\n",
      "Very familiar          0.197080\n",
      "Not familiar at all    0.036496\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LM_FREQUENCY_USE DISTRIBUTION:\n",
      "lm_frequency_use\n",
      "Once per month            0.369748\n",
      "More than once a month    0.218487\n",
      "Every week                0.201681\n",
      "Less than one a year      0.134454\n",
      "Every day                 0.075630\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LOCATION DISTRIBUTION:\n",
      "location\n",
      "US                                 0.218978\n",
      "UK                                 0.211679\n",
      "Europe                             0.175182\n",
      "Australia and New Zealand          0.109489\n",
      "Latin America and the Caribbean    0.087591\n",
      "Africa                             0.080292\n",
      "Middle East                        0.043796\n",
      "Asia                               0.043796\n",
      "Northern America                   0.029197\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "STUDY_LOCALE DISTRIBUTION:\n",
      "study_locale\n",
      "uk                0.240876\n",
      "us                0.240876\n",
      "south africa      0.058394\n",
      "australia         0.058394\n",
      "israel            0.051095\n",
      "new zealand       0.051095\n",
      "mexico            0.043796\n",
      "chile             0.043796\n",
      "canada            0.029197\n",
      "asia              0.021898\n",
      "italy             0.021898\n",
      "germany           0.014599\n",
      "poland            0.014599\n",
      "finland           0.014599\n",
      "belgium           0.014599\n",
      "latvia            0.014599\n",
      "norway            0.007299\n",
      "greece            0.007299\n",
      "austria           0.007299\n",
      "netherlands       0.007299\n",
      "slovenia          0.007299\n",
      "czech republic    0.007299\n",
      "spain             0.007299\n",
      "ireland           0.007299\n",
      "denmark           0.007299\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "RELIGION DISTRIBUTION:\n",
      "religion\n",
      "No Affiliation       0.569343\n",
      "Christian            0.313869\n",
      "Jewish               0.036496\n",
      "Muslim               0.029197\n",
      "Prefer not to say    0.029197\n",
      "Other                0.021898\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ETHNICITY DISTRIBUTION:\n",
      "ethnicity\n",
      "White                0.678832\n",
      "Asian                0.072993\n",
      "Mixed                0.065693\n",
      "Black                0.065693\n",
      "Hispanic             0.058394\n",
      "Prefer not to say    0.029197\n",
      "Other                0.029197\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== CONVERSATION CHARACTERISTICS ===\n",
      "\n",
      "NUM_TURNS:\n",
      "  Mean: 1.00\n",
      "  Median: 1.00\n",
      "  Std Dev: 0.00\n",
      "  Min: 1.00\n",
      "  Max: 1.00\n",
      "\n",
      "CONVO_LENGTH_WORDS:\n",
      "  Mean: 1508.40\n",
      "  Median: 1386.50\n",
      "  Std Dev: 619.23\n",
      "  Min: 561.00\n",
      "  Max: 3278.00\n",
      "\n",
      "AVG_TURN_LENGTH:\n",
      "  Mean: 1508.40\n",
      "  Median: 1386.50\n",
      "  Std Dev: 619.23\n",
      "  Min: 561.00\n",
      "  Max: 3278.00\n",
      "\n",
      "=== OPENING PROMPT ANALYSIS ===\n",
      "\n",
      "OPENING_PROMPT_LENGTH:\n",
      "  Mean: 50.96\n",
      "  Median: 44.00\n",
      "  Std Dev: 41.52\n",
      "  Min: 2.00\n",
      "  Max: 408.00\n",
      "\n",
      "OPENING_PROMPT_WORDS:\n",
      "  Mean: 9.90\n",
      "  Median: 8.00\n",
      "  Std Dev: 8.08\n",
      "  Min: 1.00\n",
      "  Max: 82.00\n",
      "\n",
      "=== TOPIC ANALYSIS ===\n",
      "\n",
      "Top 20 words in opening prompts:\n",
      "        frequency\n",
      "hi             16\n",
      "good           13\n",
      "like           12\n",
      "hello           9\n",
      "best            8\n",
      "make            7\n",
      "think           7\n",
      "people          7\n",
      "today           7\n",
      "need            7\n",
      "tell            6\n",
      "know            6\n",
      "doing           5\n",
      "help            5\n",
      "want            5\n",
      "day             5\n",
      "money           5\n",
      "old             5\n",
      "dinner          4\n",
      "start           4\n",
      "WordCloud package not installed. Skipping word cloud visualization.\n",
      "\n",
      "Top words per topic:\n",
      "Topic 1: hi, people, make, best, world, human, think, dinner, money, fun\n",
      "Topic 2: know, day, ai, think, like, learning, suggestions, use, recipe, recommend\n",
      "Topic 3: today, tell, hi, doing, football, want, money, story, nice, country\n",
      "Topic 4: need, time, help, united, states, good, party, birthday, ideas, book\n",
      "Topic 5: good, like, hello, write, christmas, old, weather, morning, hey, opinion\n",
      "\n",
      "=== DEMOGRAPHIC BALANCE CHECK ===\n",
      "\n",
      "Cross-tabulation of Gender and Age:\n",
      "age                        18-24 years old  25-34 years old  35-44 years old  \\\n",
      "gender                                                                         \n",
      "Female                            0.094891         0.109489         0.102190   \n",
      "Male                              0.065693         0.160584         0.087591   \n",
      "Non-binary / third gender         0.000000         0.007299         0.000000   \n",
      "\n",
      "age                        45-54 years old  55-64 years old  65+ years old  \n",
      "gender                                                                      \n",
      "Female                            0.058394         0.102190       0.029197  \n",
      "Male                              0.065693         0.072993       0.043796  \n",
      "Non-binary / third gender         0.000000         0.000000       0.000000  \n",
      "\n",
      "Cross-tabulation of Gender and Education:\n",
      "education                  Completed Secondary School  \\\n",
      "gender                                                  \n",
      "Female                                       0.109489   \n",
      "Male                                         0.043796   \n",
      "Non-binary / third gender                    0.000000   \n",
      "\n",
      "education                  Graduate / Professional degree  Some Secondary  \\\n",
      "gender                                                                      \n",
      "Female                                           0.065693        0.014599   \n",
      "Male                                             0.124088        0.007299   \n",
      "Non-binary / third gender                        0.000000        0.000000   \n",
      "\n",
      "education                  Some University but no degree  \\\n",
      "gender                                                     \n",
      "Female                                          0.051095   \n",
      "Male                                            0.102190   \n",
      "Non-binary / third gender                       0.007299   \n",
      "\n",
      "education                  University Bachelors Degree  Vocational  \n",
      "gender                                                              \n",
      "Female                                        0.211679    0.043796  \n",
      "Male                                          0.204380    0.014599  \n",
      "Non-binary / third gender                     0.000000    0.000000  \n",
      "\n",
      "Cross-tabulation of Age and Education:\n",
      "education        Completed Secondary School  Graduate / Professional degree  \\\n",
      "age                                                                           \n",
      "18-24 years old                    0.043796                        0.000000   \n",
      "25-34 years old                    0.029197                        0.051095   \n",
      "35-44 years old                    0.036496                        0.051095   \n",
      "45-54 years old                    0.007299                        0.007299   \n",
      "55-64 years old                    0.021898                        0.051095   \n",
      "65+ years old                      0.014599                        0.029197   \n",
      "\n",
      "education        Some Secondary  Some University but no degree  \\\n",
      "age                                                              \n",
      "18-24 years old        0.000000                       0.058394   \n",
      "25-34 years old        0.014599                       0.043796   \n",
      "35-44 years old        0.000000                       0.021898   \n",
      "45-54 years old        0.007299                       0.029197   \n",
      "55-64 years old        0.000000                       0.007299   \n",
      "65+ years old          0.000000                       0.000000   \n",
      "\n",
      "education        University Bachelors Degree  Vocational  \n",
      "age                                                       \n",
      "18-24 years old                     0.036496    0.021898  \n",
      "25-34 years old                     0.124088    0.014599  \n",
      "35-44 years old                     0.080292    0.000000  \n",
      "45-54 years old                     0.058394    0.014599  \n",
      "55-64 years old                     0.087591    0.007299  \n",
      "65+ years old                       0.029197    0.000000  \n",
      "\n",
      "Analysis complete! All visualizations saved to 'prism_analysis_results' folder\n",
      "Summary report saved to 'prism_analysis_results/prism_sample_analysis_summary.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "output_folder = \"prism_analysis_results\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "sample_prism = pd.read_csv('important_data_v3_metrics.csv')\n",
    "\n",
    "print(f\"PRISM Sample Dataset Shape: {sample_prism.shape}\")\n",
    "print(f\"Number of unique conversations: {sample_prism['conversation_id'].nunique()}\")\n",
    "print(f\"Number of unique participants: {sample_prism['user_id'].nunique()}\")\n",
    "\n",
    "convo_per_user = sample_prism.groupby('user_id')['conversation_id'].nunique()\n",
    "print(f\"\\nConversations per participant:\")\n",
    "print(f\"  Mean: {convo_per_user.mean():.2f}\")\n",
    "print(f\"  Median: {convo_per_user.median():.2f}\")\n",
    "print(f\"  Min: {convo_per_user.min()}\")\n",
    "print(f\"  Max: {convo_per_user.max()}\")\n",
    "\n",
    "if convo_per_user.equals(pd.Series(6, index=convo_per_user.index)):\n",
    "    print(\"âœ“ Confirmed: All participants have exactly 6 conversations.\")\n",
    "else:\n",
    "    print(\"âš  Note: Not all participants have exactly 6 conversations.\")\n",
    "    print(f\"Participants with != 6 conversations: {(convo_per_user != 6).sum()}\")\n",
    "\n",
    "demographic_vars = ['gender', 'age', 'education', 'employment_status', \n",
    "                   'marital_status', 'english_proficiency', 'lm_familiarity', \n",
    "                   'lm_frequency_use', 'location', 'study_locale', 'religion', 'ethnicity']\n",
    "\n",
    "print(\"\\n=== DEMOGRAPHIC DISTRIBUTIONS ===\")\n",
    "for demographic in demographic_vars:\n",
    "    if demographic in sample_prism.columns:\n",
    "        dist = sample_prism.groupby('user_id')[demographic].first().value_counts(normalize=True)\n",
    "        print(f\"\\n{demographic.upper()} DISTRIBUTION:\")\n",
    "        print(dist)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        dist.plot(kind='bar')\n",
    "        plt.title(f'{demographic.title()} Distribution')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{demographic}_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\n=== CONVERSATION CHARACTERISTICS ===\")\n",
    "\n",
    "if 'conversation_history' in sample_prism.columns:\n",
    "    sample_prism['num_turns'] = sample_prism['conversation_history'].str.count('\\n') + 1\n",
    "    sample_prism['convo_length_words'] = sample_prism['conversation_history'].str.split().str.len()    \n",
    "    sample_prism['avg_turn_length'] = sample_prism['convo_length_words'] / sample_prism['num_turns']\n",
    "    convo_metrics = ['num_turns', 'convo_length_words', 'avg_turn_length']\n",
    "    for metric in convo_metrics:\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        print(f\"  Mean: {sample_prism[metric].mean():.2f}\")\n",
    "        print(f\"  Median: {sample_prism[metric].median():.2f}\")\n",
    "        print(f\"  Std Dev: {sample_prism[metric].std():.2f}\")\n",
    "        print(f\"  Min: {sample_prism[metric].min():.2f}\")\n",
    "        print(f\"  Max: {sample_prism[metric].max():.2f}\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(sample_prism[metric], kde=True)\n",
    "        plt.title(f'Distribution of {metric.replace(\"_\", \" \").title()}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{metric}_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "if 'opening_prompt' in sample_prism.columns:\n",
    "    print(\"\\n=== OPENING PROMPT ANALYSIS ===\")\n",
    "    sample_prism['opening_prompt_length'] = sample_prism['opening_prompt'].str.len()\n",
    "    sample_prism['opening_prompt_words'] = sample_prism['opening_prompt'].str.split().str.len()\n",
    "    \n",
    "    prompt_metrics = ['opening_prompt_length', 'opening_prompt_words']\n",
    "    for metric in prompt_metrics:\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        print(f\"  Mean: {sample_prism[metric].mean():.2f}\")\n",
    "        print(f\"  Median: {sample_prism[metric].median():.2f}\")\n",
    "        print(f\"  Std Dev: {sample_prism[metric].std():.2f}\")\n",
    "        print(f\"  Min: {sample_prism[metric].min():.2f}\")\n",
    "        print(f\"  Max: {sample_prism[metric].max():.2f}\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(sample_prism[metric], kde=True)\n",
    "        plt.title(f'Distribution of {metric.replace(\"_\", \" \").title()}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{metric}_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "if 'opening_prompt' in sample_prism.columns:\n",
    "    print(\"\\n=== TOPIC ANALYSIS ===\")\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features=1000)\n",
    "    dtm = vectorizer.fit_transform(sample_prism['opening_prompt'].fillna(''))\n",
    "    \n",
    "    # Simple word frequency analysis\n",
    "    word_freq = pd.DataFrame(dtm.sum(axis=0), \n",
    "                           columns=vectorizer.get_feature_names_out()).T\n",
    "    word_freq.columns = ['frequency']\n",
    "    top_words = word_freq.sort_values('frequency', ascending=False).head(20)\n",
    "    \n",
    "    print(\"\\nTop 20 words in opening prompts:\")\n",
    "    print(top_words)\n",
    "    \n",
    "    try:\n",
    "        from wordcloud import WordCloud\n",
    "        \n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(\n",
    "            word_freq['frequency'].to_dict())\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, 'wordcloud_topics.png'))\n",
    "        plt.close()\n",
    "    except ImportError:\n",
    "        print(\"WordCloud package not installed. Skipping word cloud visualization.\")\n",
    "        \n",
    "    try:\n",
    "        lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "        lda.fit(dtm)\n",
    "        \n",
    "        print(\"\\nTop words per topic:\")\n",
    "        features = vectorizer.get_feature_names_out()\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_features_idx = topic.argsort()[:-11:-1]\n",
    "            top_features = [features[i] for i in top_features_idx]\n",
    "            print(f\"Topic {topic_idx+1}: {', '.join(top_features)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Topic modeling skipped: {str(e)}\")\n",
    "\n",
    "print(\"\\n=== DEMOGRAPHIC BALANCE CHECK ===\")\n",
    "for demo1, demo2 in [('gender', 'age'), ('gender', 'education'), ('age', 'education')]:\n",
    "    if demo1 in sample_prism.columns and demo2 in sample_prism.columns:\n",
    "        user_demos = sample_prism.groupby('user_id')[[demo1, demo2]].first()\n",
    "        \n",
    "        cross_tab = pd.crosstab(user_demos[demo1], user_demos[demo2], normalize='all')\n",
    "        \n",
    "        print(f\"\\nCross-tabulation of {demo1.title()} and {demo2.title()}:\")\n",
    "        print(cross_tab)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cross_tab, annot=True, cmap='YlGnBu', fmt='.2%')\n",
    "        plt.title(f'Distribution of Participants: {demo1.title()} vs {demo2.title()}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{demo1}_{demo2}_heatmap.png'))\n",
    "        plt.close()\n",
    "\n",
    "if 'lm_familiarity' in sample_prism.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    user_lm_familiarity = sample_prism.groupby('user_id')['lm_familiarity'].first()\n",
    "    \n",
    "    plt.pie(user_lm_familiarity.value_counts(), \n",
    "            labels=user_lm_familiarity.value_counts().index,\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('LLM Familiarity Distribution')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'lm_familiarity_pie.png'))\n",
    "    plt.close()\n",
    "\n",
    "with open(os.path.join(output_folder, 'prism_sample_analysis_summary.txt'), 'w') as f:\n",
    "    f.write(\"PRISM Sample Analysis Summary\\n\")\n",
    "    f.write(\"============================\\n\\n\")\n",
    "    f.write(f\"Dataset size: {sample_prism.shape[0]} rows\\n\")\n",
    "    f.write(f\"Unique participants: {sample_prism['user_id'].nunique()}\\n\")\n",
    "    f.write(f\"Unique conversations: {sample_prism['conversation_id'].nunique()}\\n\")\n",
    "    f.write(f\"Average conversations per participant: {convo_per_user.mean():.2f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Key demographic distributions:\\n\")\n",
    "    for demographic in ['gender', 'age', 'education']:\n",
    "        if demographic in sample_prism.columns:\n",
    "            f.write(f\"\\n{demographic.upper()}:\\n\")\n",
    "            dist = sample_prism.groupby('user_id')[demographic].first().value_counts(normalize=True)\n",
    "            for category, value in dist.items():\n",
    "                f.write(f\"  {category}: {value*100:.1f}%\\n\")\n",
    "    \n",
    "    f.write(\"\\nVisualization files generated:\\n\")\n",
    "    for demo in demographic_vars:\n",
    "        if demo in sample_prism.columns:\n",
    "            f.write(f\"- {demo}_distribution.png\\n\")\n",
    "    \n",
    "    for metric in ['num_turns', 'convo_length_words', 'avg_turn_length', 'opening_prompt_length', 'opening_prompt_words']:\n",
    "        f.write(f\"- {metric}_distribution.png\\n\")\n",
    "    \n",
    "    for pair in [('gender', 'age'), ('gender', 'education'), ('age', 'education')]:\n",
    "        demo1, demo2 = pair\n",
    "        if demo1 in sample_prism.columns and demo2 in sample_prism.columns:\n",
    "            f.write(f\"- {demo1}_{demo2}_heatmap.png\\n\")\n",
    "\n",
    "print(f\"\\nAnalysis complete! All visualizations saved to '{output_folder}' folder\")\n",
    "print(f\"Summary report saved to '{os.path.join(output_folder, 'prism_sample_analysis_summary.txt')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"hf://datasets/HannahRoseKirk/prism-alignment/survey.jsonl\", lines=True)\n",
    "sample_df = pd.read_csv('important_data_v3_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'survey_only', 'num_completed_conversations', 'consent',\n",
      "       'consent_age', 'lm_familiarity', 'lm_indirect_use', 'lm_direct_use',\n",
      "       'lm_frequency_use', 'self_description', 'system_string', 'age',\n",
      "       'gender', 'employment_status', 'education', 'marital_status',\n",
      "       'english_proficiency', 'study_id', 'study_locale', 'religion',\n",
      "       'ethnicity', 'location', 'lm_usecases', 'stated_prefs',\n",
      "       'order_lm_usecases', 'order_stated_prefs', 'generated_datetime',\n",
      "       'timing_duration_s', 'timing_duration_mins', 'included_in_US_REP',\n",
      "       'included_in_UK_REP', 'included_in_balanced_subset'],\n",
      "      dtype='object')\n",
      "Index(['conversation_id', 'user_id', 'opening_prompt', 'conversation_history',\n",
      "       'lm_familiarity', 'lm_frequency_use', 'age', 'gender',\n",
      "       'employment_status', 'education', 'marital_status',\n",
      "       'english_proficiency', 'study_locale', 'religion', 'ethnicity',\n",
      "       'location', 'human_lang', 'llm_lang', 'human_flesch_reading_ease',\n",
      "       'human_flesch_kincaid_grade', 'human_smog_index', 'human_ari',\n",
      "       'human_lexical_diversity', 'human_tb_polarity', 'human_tb_subjectivity',\n",
      "       'human_vader_neg', 'human_vader_neu', 'human_vader_pos',\n",
      "       'human_vader_compound', 'llm_flesch_reading_ease',\n",
      "       'llm_flesch_kincaid_grade', 'llm_smog_index', 'llm_ari',\n",
      "       'llm_lexical_diversity', 'llm_tb_polarity', 'llm_tb_subjectivity',\n",
      "       'llm_vader_neg', 'llm_vader_neu', 'llm_vader_pos', 'llm_vader_compound',\n",
      "       'score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(sample_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
